{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/Tweets.csv')\ndf = data[['text']]","execution_count":31,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenization to sentences\nGiven text is split into individual sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"welcome readers. Good muffins cost 3.88. I am a good boy and my name is unknown. I hope you find it interesting isn't it \"","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_tokenize(text)","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"['welcome readers.',\n 'Good muffins cost 3.88.',\n 'I am a good boy and my name is unknown.',\n \"I hope you find it interesting isn't it\"]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nn = []\nfor p in df['text']:\n    l = sent_tokenize(p)\n    n.append(l)\n\ndf['text2'] = n","execution_count":35,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 10.5 µs\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ndf['text2'] = df.text.apply(sent_tokenize)","execution_count":36,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 11.4 µs\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"There is a tokenizer called punkt tokenizer (Unsupervised Multilingual Sentence Boundary Detection paper by kiss and strunk). It is based on the assumption that a large number of ambiguities in the determination of sentence boundaries can be eliminated once abbreviations have been identified"},{"metadata":{},"cell_type":"markdown","source":"## Sentences into words"},{"metadata":{},"cell_type":"markdown","source":"The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.This is the method that is invoked by `word_tokenize()`.  It assumes that the text has already been segmented into sentences"},{"metadata":{},"cell_type":"markdown","source":"This tokenizer performs the following steps:\n    - split standard contractions, e.g. ``don't`` -> ``do n't`` and ``they'll`` -> ``they 'll``\n    - treat most punctuation characters as separate tokens\n    - split off commas and single quotes, when followed by whitespace\n    - separate periods that appear at the end of line\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"welcome readers. Good muffins cost 3.88. I am a good boy and my name is unknown. I hope you find it interesting isn't it \"","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nword_tokenize(text)","execution_count":38,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 10 µs\n","name":"stdout"},{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"['welcome',\n 'readers',\n '.',\n 'Good',\n 'muffins',\n 'cost',\n '3.88',\n '.',\n 'I',\n 'am',\n 'a',\n 'good',\n 'boy',\n 'and',\n 'my',\n 'name',\n 'is',\n 'unknown',\n '.',\n 'I',\n 'hope',\n 'you',\n 'find',\n 'it',\n 'interesting',\n 'is',\n \"n't\",\n 'it']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ndf[\"text3\"] = df.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)","execution_count":39,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 10 µs\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ndf[\"text4\"] = df.text.apply(word_tokenize)","execution_count":40,"outputs":[{"output_type":"stream","text":"CPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 10 µs\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Using WordPunct Tokenizer"},{"metadata":{},"cell_type":"markdown","source":"By using wordpunct tokenizer we are able to extract the token from strings of words or sentences in form of alphabatic or non-alphabatic character"},{"metadata":{},"cell_type":"markdown","source":"It provides splitting by making punctuation an entirely new token"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import WordPunctTokenizer\ntokenizer = WordPunctTokenizer()","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"don't hesitate to ask questions\"","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.tokenize(text)","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"['don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize using Reg exp"},{"metadata":{},"cell_type":"markdown","source":"Tokenization of words performed by constructing regular expressions in two ways\n* matching with words\n* matching spaces or gaps"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\n\ntok1 = RegexpTokenizer('\\s+',gaps=True)\ntok2 = RegexpTokenizer('[A-Z]\\w+')","execution_count":59,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* splits a string into substrings using a regular expression\n* Explicitly defined pattern (pos argument"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tok1.tokenize(\"You're handsome Isn't it\"))\nprint(tok2.tokenize(\"You're handsome Isn't it\"))","execution_count":60,"outputs":[{"output_type":"stream","text":"[\"You're\", 'handsome', \"Isn't\", 'it']\n['You', 'Isn']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"* It uses re.findall( to perform tokenization by matching tokens\n* re.split to perform tokenization by matching gaps or spaces"},{"metadata":{},"cell_type":"markdown","source":"## Line and Space Tokenizer\n\n* useful for removing newline have only spaces \n* similar to text.split(\\n)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import LineTokenizer, WhitespaceTokenizer","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"Happy birthday to you \\n \\n. Brother.\"","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(LineTokenizer(blanklines='keep').tokenize(text))\nprint(LineTokenizer(blanklines='discard').tokenize(text))","execution_count":83,"outputs":[{"output_type":"stream","text":"['Happy birthday to you ', ' ', '. Brother.']\n['Happy birthday to you ', '. Brother.']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"* Tokenize a string on whitespace (space, tab, newline). In general, users should use the string ``split()`` method instead\n* Space tokenizer work as text.split()"},{"metadata":{"trusted":true},"cell_type":"code","source":"WhitespaceTokenizer().tokenize(text)","execution_count":84,"outputs":[{"output_type":"execute_result","execution_count":84,"data":{"text/plain":"['Happy', 'birthday', 'to', 'you', '.', 'Brother.']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}